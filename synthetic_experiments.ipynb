{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe70ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T16:08:33.845190Z",
     "start_time": "2023-02-10T16:08:30.779708Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import os.path\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89963d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:27:27.694674Z",
     "start_time": "2023-02-11T04:27:27.669666Z"
    }
   },
   "outputs": [],
   "source": [
    "def round_p(value, round_digits):\n",
    "    return format(round(value,round_digits), \".\"+str(round_digits)+\"f\")\n",
    "def mse_loss(beta_, c, d, y, t):\n",
    "    return(c/(1+np.exp(-beta_.dot(t)))+d-y)**2\n",
    "\n",
    "def avg_y(beta_, c, d, t):\n",
    "    return c/(1+np.exp(-beta_.dot(t)))+d\n",
    "\n",
    "def exp(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "def generate_y(coef, c, d, x, t, n):\n",
    "    y = np.zeros(n)\n",
    "    y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "    for i in range(n):\n",
    "        y[i] = c/(1+np.exp(-(x[i].dot(coef)).dot(t[i]))) + d + y_error[i]\n",
    "    return y, y_error\n",
    "\n",
    "def generate_x(d_c, n):\n",
    "    return np.random.uniform(0, 1, [n, d_c])\n",
    "\n",
    "def generate_t(t_combo, t_dist, n):\n",
    "    return t_combo[np.random.choice(np.shape(t_dist)[0], n, p=t_dist),]\n",
    "\n",
    "\n",
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]\n",
    "\n",
    "def calculate_mse(loader, is_gpu, net):\n",
    "    \"\"\"Calculate accuracy.\n",
    "\n",
    "    Args:\n",
    "        loader (torch.utils.data.DataLoader): training / test set loader\n",
    "        is_gpu (bool): whether to run on GPU\n",
    "    Returns:\n",
    "        tuple: (overall accuracy, class level accuracy)\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        \n",
    "        cnt += labels.size(0)\n",
    "        total_loss += sum((outputs-labels)**2)\n",
    "\n",
    "    return total_loss/float(cnt)\n",
    "\n",
    "def plot_dev(loader, is_gpu, net):\n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    real_y = []\n",
    "    pred_y = []\n",
    "\n",
    "    for data in loader:\n",
    "        inputs, labels = data\n",
    "        if is_gpu:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        real_y.append(labels.tolist())\n",
    "        pred_y.append(outputs.tolist())\n",
    "        \n",
    "        cnt += labels.size(0)\n",
    "        total_loss += sum((outputs-labels)**2)\n",
    "    real_y = [x for sublist in real_y for x in sublist]\n",
    "    pred_y = [x for sublist in pred_y for x in sublist]\n",
    "    print('MSELoss= ', total_loss/float(cnt))\n",
    "    plt.scatter(real_y, pred_y)\n",
    "    plt.xlabel('real y')\n",
    "    plt.ylabel('pred y')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# hyperparameters settings\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--wd', type=float, default=5e-4, help='weight decay')#lr/(c+wd)\n",
    "parser.add_argument('--epochs', type=int, default=50,\n",
    "                    help='number of epochs to train')\n",
    "parser.add_argument('--batch_size_train', type=int,\n",
    "                    default=1000, help='training set input batch size')\n",
    "parser.add_argument('--batch_size_test', type=int,\n",
    "                    default=1000, help='test set input batch size')\n",
    "parser.add_argument('--is_gpu', type=bool, default=False,\n",
    "                    help='whether training using GPU')\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f61e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:27:31.019119Z",
     "start_time": "2023-02-11T04:27:31.017447Z"
    }
   },
   "outputs": [],
   "source": [
    "n_rep = 5#we replicate 200 times in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfed773",
   "metadata": {},
   "source": [
    "# Validation of DeDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a16db9",
   "metadata": {},
   "source": [
    "## Data generating process definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946056b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:27:34.418396Z",
     "start_time": "2023-02-11T04:27:34.411017Z"
    }
   },
   "outputs": [],
   "source": [
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "lr = 0.05\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "n_est = int(m*500)\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "#reg_loss = 0.001\n",
    "reg_loss = 0\n",
    "train_epochs = 2000\n",
    "test_thres = 0.3\n",
    "n_cnver_test = 10000\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "true_1 = []#real ATE in treatment\n",
    "\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "\n",
    "estimator_0 = []#ATE by SDL in base\n",
    "estimator_1 = []#ATE by SDL in treatment\n",
    "\n",
    "estimator_debias_0 = []#ATE by DeDL in base\n",
    "estimator_debias_1 = []#ATE by DeDL in treatment\n",
    "\n",
    "estimator_additive_1 = []#ATE by LA in treatment\n",
    "mse_c = []\n",
    "mse_theta = []\n",
    "mae_theta = []\n",
    "\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_DL = []#p value by SDL\n",
    "p_DDL = []#p value by DeDL\n",
    "test_err = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139293f3",
   "metadata": {},
   "source": [
    "## Comparison of LA, LR, SDL, DeDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c19db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:27:37.723447Z",
     "start_time": "2023-02-11T04:27:37.719124Z"
    }
   },
   "outputs": [],
   "source": [
    "# FNN+poly with k=4 and 4 exps\n",
    "class FNN_asig(nn.Module):\n",
    "    \"\"\"FNN.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"FNN Builder.\"\"\"\n",
    "        super(FNN_asig, self).__init__()\n",
    "        \n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(d_c, 10, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(10, m+1)\n",
    "        )\n",
    "        self.siglayer = nn.Sigmoid()\n",
    "        self.layer3 = nn.Linear(1, 1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        b = self.layer1(x[:,0:d_c])\n",
    "        u = torch.sum(b*x[:, d_c:], 1)\n",
    "        u = self.siglayer(u)\n",
    "        u = u.unsqueeze(1)\n",
    "        u = self.layer3(u)\n",
    "        return torch.reshape(u, (-1,))\n",
    "    \n",
    "def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036fb85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:29:40.835514Z",
     "start_time": "2023-02-11T04:27:41.025676Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rep_index = 0\n",
    "\n",
    "while rep_index < n_rep:\n",
    "    \n",
    "    print('# of replication:', rep_index)\n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "\n",
    "    def generate_y_true(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            y[i] = c/(1+np.exp(-((x[i].dot(coef))**3).dot(t[i]))) + d + y_error[i]\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef, x, t):\n",
    "        return c_true/(1+np.exp(-((x.dot(coef))**3).dot(t)))\n",
    "    def generate_beta_true_1(coef, x, t):\n",
    "        return (x.dot(coef))**3\n",
    "    \n",
    "    #generate samples for estimation\n",
    "    samples_x_est = generate_x(d_c, n_train)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_train)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_train)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_train,1), axis=1)\n",
    "    data_obs = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "    \n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_train,1), np.append(samples_x_est, samples_t_est, axis=1))\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    train_samples = int(0.7*data_obs.shape[0])\n",
    "    x_train_set = np.float32(data_obs)[0:train_samples, 0:-1]\n",
    "    y_train_set = np.float32(data_obs)[0:train_samples, -1]\n",
    "    x_test_set = np.float32(data_obs)[train_samples:, 0:-1]\n",
    "    y_test_set = np.float32(data_obs)[train_samples:, -1]\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=opt.batch_size_test, shuffle=True)\n",
    "    \n",
    "    net = FNN_asig()\n",
    "    test_accuracy = 100\n",
    "    print('---------warm-up train---------')\n",
    "    wp_cnt = 0\n",
    "    while(test_accuracy >= 1):\n",
    "        wp_cnt += 1\n",
    "        net = FNN_asig()\n",
    "        with torch.no_grad():\n",
    "            net.layer3.weight[0, 0] = float(np.max(y_train_set))\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.L1Loss()\n",
    "        #warm-up train\n",
    "        test_accuracy = 100\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.1, weight_decay = opt.wd)\n",
    "        for epoch in range(200):\n",
    "            for i, data_i in enumerate(trainloader, 0):\n",
    "                inputs, labels = data_i\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "                loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "            test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "            if test_accuracy < 1:\n",
    "                break\n",
    "        if wp_cnt >= 5:\n",
    "            break\n",
    "    #training \n",
    "    print('---------train---------')\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = opt.wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(train_epochs):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "        test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "                epoch, train_accuracy, test_accuracy))\n",
    "        if test_accuracy < test_thres and epoch > 500:\n",
    "            break  \n",
    "    if test_accuracy > test_thres:\n",
    "        continue\n",
    "    else:\n",
    "        rep_index += 1\n",
    "        test_err.append(test_accuracy)\n",
    "    \n",
    "    activation = {}\n",
    "    net.layer1.register_forward_hook(get_activation('layer1'))\n",
    "    \n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est,1), axis=1)\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "        \n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        beta_ = []\n",
    "        pred_y_loss = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            beta_.append(activation['layer1'].tolist())  \n",
    "            pred_y_loss.append(outputs.tolist())\n",
    "        beta_ = np.array(beta_).reshape(n_est, m+1)\n",
    "        pred_y_loss = np.array(pred_y_loss).reshape(n_est)\n",
    "        \n",
    "        real_y = samples_y_est.copy()\n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_combo_obs.shape[0]):\n",
    "                t = t_combo_obs[i]\n",
    "                y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                u = beta_temp.dot(t)\n",
    "                G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))#LR\n",
    "        estimator_0.append(np.mean(pred_y))#SDL\n",
    "        estimator_debias_0.append(np.mean(pred_y_debiased))#DeDL\n",
    "        true_0.append(np.mean(real_y_star))#ground truth\n",
    "        \n",
    "        #record for comparison - t test\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        pred_y_debiased_0 = pred_y_debiased.copy()\n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        for i in range(n_est):\n",
    "            for j in range(m):\n",
    "                x_all_set[i][-m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "\n",
    "        G_theta = []\n",
    "        \n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_1.append(np.mean(real_y_star))\n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        estimator_1.append(np.mean(pred_y))\n",
    "        estimator_debias_1.append(np.mean(pred_y_debiased))\n",
    "\n",
    "        p_.append(stats.ttest_ind(real_y_star, real_y_star_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1]) \n",
    "        p_DL.append(stats.ttest_ind(pred_y, pred_y_0)[1])\n",
    "        p_DDL.append(stats.ttest_ind(pred_y_debiased, pred_y_debiased_0)[1])\n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67de56b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:29:52.624292Z",
     "start_time": "2023-02-11T04:29:52.579177Z"
    }
   },
   "outputs": [],
   "source": [
    "test_err_np = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    test_err_np[i] = test_err[i].tolist()\n",
    "test_err =  test_err_np.copy()\n",
    "del test_err_np\n",
    "\n",
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_DL = np.array(p_DL).reshape((n_rep, 2**m))\n",
    "p_DDL = np.array(p_DDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_0 = np.array(estimator_0).reshape((n_rep, 2**m))\n",
    "estimator_debias_0 = np.array(estimator_debias_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_1 = np.array(estimator_1).reshape((n_rep, 2**m))\n",
    "estimator_debias_1 = np.array(estimator_debias_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR SDL DeDL\n",
    "ape_2 = np.zeros([2**m, 4, n_rep])\n",
    "se = np.zeros([2**m, 4, n_rep])\n",
    "ae = np.zeros([2**m, 4, n_rep])\n",
    "CDR = np.zeros([2**m, 4, n_rep])\n",
    "for i in range(n_rep):\n",
    "    CDR[0,:,i] = 1\n",
    "    for j in range(1,2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        dl = estimator_1[i,j]-estimator_0[i,j]\n",
    "        dedl = estimator_debias_1[i,j]-estimator_debias_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DL[i,j]<=0.05 and dl*true_eff>0) or (p_[i,j]>0.05 and p_DL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DDL[i,j]<=0.05 and dedl*true_eff>0) or (p_[i,j]>0.05 and p_DDL[i,j]>0.05):\n",
    "            CDR[j,3,i] = 1\n",
    "            \n",
    "\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]    \n",
    "    no_bias_effect = estimator_1[:,index]-estimator_0[:,index]\n",
    "    debias_effect = estimator_debias_1[:,index]-estimator_debias_0[:,index]\n",
    "    \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((no_bias_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 3, :] = np.abs((debias_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 3, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)    \n",
    "    se[index, 2, :] = ((no_bias_effect - true_effect)**2)\n",
    "    se[index, 3, :] = ((debias_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((no_bias_effect - true_effect))\n",
    "    ae[index, 3, :] = np.abs((debias_effect - true_effect))\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of SDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('MSE of DeDL         ','|', np.mean(se[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,3,:], axis=0)), scale=st.sem(np.mean(se[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of SDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('MAE of DeDL         ','|', np.mean(ae[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,3,:], axis=0)), scale=st.sem(np.mean(ae[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE of LA          ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of LR          ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of SDL         ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of DeDL        ','|', np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of SDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('CDR of DeDL         ','|', np.mean(CDR[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,3,:], axis=0)), scale=st.sem(np.mean(CDR[:,3,:], axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 4])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(5)\n",
    "    max_effect_index = np.zeros(5)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        no_bias_effect = estimator_1[i,index]-estimator_0[i,index]\n",
    "        debias_effect = estimator_debias_1[i,index]-estimator_debias_0[i,index]\n",
    "        \n",
    "        if true_effect > max_effect[4]:\n",
    "            max_effect[4] = true_effect\n",
    "            max_effect_index[4] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if no_bias_effect > max_effect[2]:\n",
    "            max_effect[2] = no_bias_effect\n",
    "            max_effect_index[2] = index\n",
    "        if debias_effect > max_effect[3]:\n",
    "            max_effect[3] = debias_effect\n",
    "            max_effect_index[3] = index\n",
    "        \n",
    "    if max_effect_index[0] == max_effect_index[4]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[4]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[4]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "    if max_effect_index[3] == max_effect_index[4]:\n",
    "        cnt_bai[i, 3] = 1\n",
    "    \n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of SDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('BAI of DeDL         ','|', np.mean(cnt_bai[:, 3]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 3]), scale=st.sem(cnt_bai[:, 3])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2ab8e",
   "metadata": {},
   "source": [
    "## PDL Performance under varying NN size and partially/all observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbcb6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:29:58.501841Z",
     "start_time": "2023-02-11T04:29:58.494969Z"
    }
   },
   "outputs": [],
   "source": [
    "is_small_dnn = 1#by default for fair comparison\n",
    "is_all_obs = 0#by default for fair comparison\n",
    "\n",
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "is_all_obs = 1\n",
    "is_small_dnn = 0\n",
    "n_est = int(m*500)\n",
    "\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "#reg_loss = 0.05\n",
    "reg_loss = 0\n",
    "train_epochs = 1000\n",
    "test_thres = 1.0\n",
    "\n",
    "lr = 0.05\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "true_1 = []#real ATE in treatment\n",
    "\n",
    "estimator_0 = []#ATE by PDL in base\n",
    "estimator_1 = []#ATE by PDL in treatment\n",
    "\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "estimator_PDL_0 = []#ATE by PDL in base\n",
    "estimator_PDL_1 = []#ATE by PDL in treatment\n",
    "\n",
    "estimator_additive_1 = []#ATE by additive in treatment\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_PDL = []\n",
    "\n",
    "test_err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18e6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:30:04.300330Z",
     "start_time": "2023-02-11T04:30:04.294633Z"
    }
   },
   "outputs": [],
   "source": [
    "if is_small_dnn == 0:\n",
    "    \n",
    "    class FNN_PDL(nn.Module):\n",
    "        \"\"\"FNN.\"\"\"\n",
    "\n",
    "        def __init__(self, xavier_init = None):\n",
    "            \"\"\"FNN Builder.\"\"\"\n",
    "            super(FNN_PDL, self).__init__()\n",
    "\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Linear(d_c+m+1, 40, bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(40, 40),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(40, 1)\n",
    "            )\n",
    "\n",
    "            if (xavier_init is not None):\n",
    "                for m_ in self.modules():\n",
    "                    if isinstance(m_, nn.Linear):\n",
    "                        nn.init.xavier_normal_(m_.weight)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Perform forward.\"\"\"\n",
    "            x = self.layer1(x)     \n",
    "            return torch.reshape(x, (-1,))\n",
    "\n",
    "else:\n",
    "    \n",
    "    class FNN_PDL(nn.Module):\n",
    "        \"\"\"FNN.\"\"\"\n",
    "\n",
    "        def __init__(self, xavier_init = None):\n",
    "            \"\"\"FNN Builder.\"\"\"\n",
    "            super(FNN_PDL, self).__init__()\n",
    "\n",
    "\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Linear(d_c+m+1, 10, bias=False),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(10, 10),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(10, 1)\n",
    "            )\n",
    "\n",
    "            if (xavier_init is not None):\n",
    "                for m_ in self.modules():\n",
    "                    if isinstance(m_, nn.Linear):\n",
    "                        nn.init.xavier_normal_(m_.weight)\n",
    "\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"Perform forward.\"\"\"\n",
    "            x = self.layer1(x)     \n",
    "            return torch.reshape(x, (-1,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252fc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:31:01.842736Z",
     "start_time": "2023-02-11T04:30:10.106485Z"
    }
   },
   "outputs": [],
   "source": [
    "rep_index = 0\n",
    "\n",
    "while rep_index < n_rep:\n",
    "    \n",
    "    print('# of replication:', rep_index)\n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "\n",
    "    def generate_y_true(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            y[i] = c/(1+np.exp(-((x[i].dot(coef))**3).dot(t[i]))) + d + y_error[i]\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef, x, t):\n",
    "        return c_true/(1+np.exp(-((x.dot(coef))**3).dot(t)))\n",
    "    def generate_beta_true_1(coef, x, t):\n",
    "        return (x.dot(coef))**3\n",
    "    \n",
    "    #generate samples for estimation\n",
    "    samples_x_est = generate_x(d_c, n_train)\n",
    "    if is_all_obs == 0:\n",
    "        samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_train)\n",
    "    else:\n",
    "        samples_t_est = generate_t(t_combo, t_dist, n_train)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_train)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_train,1), axis=1)\n",
    "    data_obs = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "    \n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_train,1), np.append(samples_x_est, samples_t_est, axis=1))\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "    #results.bse\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    train_samples = int(0.7*data_obs.shape[0])\n",
    "    x_train_set = np.float32(data_obs)[0:train_samples, 0:-1]\n",
    "    y_train_set = np.float32(data_obs)[0:train_samples, -1]\n",
    "    x_test_set = np.float32(data_obs)[train_samples:, 0:-1]\n",
    "    y_test_set = np.float32(data_obs)[train_samples:, -1]\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=opt.batch_size_test, shuffle=True)\n",
    "    \n",
    "    net = FNN_PDL(xavier_init=1)\n",
    "    test_accuracy = 100\n",
    "    print('---------warm-up train---------')\n",
    "    wp_cnt = 0\n",
    "    while(test_accuracy >= 1):\n",
    "        wp_cnt += 1\n",
    "        net = FNN_PDL()\n",
    "        criterion = nn.MSELoss()\n",
    "        test_accuracy = 100\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.1, weight_decay = opt.wd)\n",
    "        for epoch in range(200):\n",
    "            for i, data_i in enumerate(trainloader, 0):\n",
    "                inputs, labels = data_i\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "                loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "            test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "            #if epoch%100 == 0:\n",
    "            #    print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "            #        epoch, train_accuracy, test_accuracy))\n",
    "            if test_accuracy < 5:\n",
    "                break\n",
    "        if wp_cnt >= 5:\n",
    "            break\n",
    "    #training \n",
    "    print('---------train---------')\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = opt.wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(train_epochs):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "        test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "                epoch, train_accuracy, test_accuracy))\n",
    "        if test_accuracy < test_thres and epoch > 200:\n",
    "            break  \n",
    "    if test_accuracy > test_thres:\n",
    "        continue\n",
    "    else:\n",
    "        rep_index += 1\n",
    "        test_err.append(test_accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est,1), axis=1)\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        \n",
    "        \n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "                \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "        \n",
    "        \n",
    "        true_0.append(np.mean(real_y_star))\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))\n",
    "        estimator_PDL_0.append(np.mean(pred_y))\n",
    "\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "        \n",
    "        \n",
    "        true_1.append(np.mean(real_y_star))\n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        estimator_PDL_1.append(np.mean(pred_y))\n",
    "        p_.append(stats.ttest_ind(real_y_star, real_y_star_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1]) \n",
    "        p_PDL.append(stats.ttest_ind(pred_y, pred_y_0)[1]) \n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615ff48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:31:07.759253Z",
     "start_time": "2023-02-11T04:31:07.690743Z"
    }
   },
   "outputs": [],
   "source": [
    "test_err_np = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    test_err_np[i] = test_err[i].tolist()\n",
    "test_err =  test_err_np.copy()\n",
    "del test_err_np\n",
    "\n",
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_PDL = np.array(p_PDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_PDL_0 = np.array(estimator_PDL_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_PDL_1 = np.array(estimator_PDL_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR PDL\n",
    "ape_2 = np.zeros([2**m, 3, n_rep])\n",
    "se = np.zeros([2**m, 3, n_rep])\n",
    "ae = np.zeros([2**m, 3, n_rep])\n",
    "CDR = np.zeros([2**m, 3, n_rep])\n",
    "for i in range(n_rep):\n",
    "    for j in range(2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        pdl = estimator_PDL_1[i,j]-estimator_PDL_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_PDL[i,j]<=0.05 and pdl*true_eff>0) or (p_[i,j]>0.05 and p_PDL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    \n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]\n",
    "    PDL_effect = estimator_PDL_1[:,index]-estimator_PDL_0[:,index]\n",
    "   \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((PDL_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)\n",
    "    se[index, 2, :] = ((PDL_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((PDL_effect - true_effect))\n",
    "    \n",
    "    \n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of PDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of PDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE2 of LA         ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of LR         ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of PDL        ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of PDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 3])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(4)\n",
    "    max_effect_index = np.zeros(4)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        PDL_effect = estimator_PDL_1[i,index]-estimator_PDL_0[i,index]\n",
    "                \n",
    "        if true_effect > max_effect[3]:\n",
    "            max_effect[3] = true_effect\n",
    "            max_effect_index[3] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if PDL_effect > max_effect[2]:\n",
    "            max_effect[2] = PDL_effect\n",
    "            max_effect_index[2] = index\n",
    "    if max_effect_index[0] == max_effect_index[3]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[3]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[3]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "\n",
    "\n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of PDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))\n",
    "\n",
    "\n",
    "print('----------------observed combos--------------------')\n",
    "\n",
    "index = 0\n",
    "index_1 = 0\n",
    "ape_2 = np.zeros([2**m-np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "se = np.zeros([2**m-np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "ae = np.zeros([2**m-np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "\n",
    "\n",
    "for t_star in t_star_all:\n",
    "    if list(t_star) not in t_star_unobs.tolist():\n",
    "        #print('Unobserved treatment effect when t = ', t_star[1:])\n",
    "    \n",
    "        true_effect = true_1[:,index]-true_0[:,index]\n",
    "    \n",
    "        add_effect = estimator_additive_1[:,index]\n",
    "        LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]\n",
    "        PDL_effect = estimator_PDL_1[:,index]-estimator_PDL_0[:,index]\n",
    "\n",
    "        ape_2[index_1, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "        ape_2[index_1, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "        ape_2[index_1, 2, :] = np.abs((PDL_effect-true_effect))/(np.abs(true_effect))\n",
    "\n",
    "        #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "        ape_2[index_1, 0, p_[:,index]>=0.05] = 0\n",
    "        ape_2[index_1, 1, p_[:,index]>=0.05] = 0\n",
    "        ape_2[index_1, 2, p_[:,index]>=0.05] = 0\n",
    "\n",
    "        se[index_1, 0, :] = ((add_effect - true_effect)**2)\n",
    "        se[index_1, 1, :] = ((LR_effect - true_effect)**2)\n",
    "        se[index_1, 2, :] = ((PDL_effect - true_effect)**2)\n",
    "\n",
    "        ae[index_1, 0, :] = np.abs((add_effect - true_effect))\n",
    "        ae[index_1, 1, :] = np.abs((LR_effect - true_effect))\n",
    "        ae[index_1, 2, :] = np.abs((PDL_effect - true_effect))\n",
    "    \n",
    "    \n",
    "        index_1 += 1\n",
    "\n",
    "    index += 1\n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of PDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of PDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE2 of LA         ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of LR         ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of PDL        ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('----------------unobserved combos--------------------')\n",
    "\n",
    "index = 0\n",
    "index_1 = 0\n",
    "ape_2 = np.zeros([np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "se = np.zeros([np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "ae = np.zeros([np.shape(t_star_unobs)[0], 3, n_rep])\n",
    "\n",
    "\n",
    "for t_star in t_star_all:\n",
    "    if list(t_star) in t_star_unobs.tolist():\n",
    "        #print('Unobserved treatment effect when t = ', t_star[1:])\n",
    "    \n",
    "        true_effect = true_1[:,index]-true_0[:,index]\n",
    "    \n",
    "        add_effect = estimator_additive_1[:,index]\n",
    "        LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]\n",
    "        PDL_effect = estimator_PDL_1[:,index]-estimator_PDL_0[:,index]\n",
    "\n",
    "        ape_2[index_1, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "        ape_2[index_1, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "        ape_2[index_1, 2, :] = np.abs((PDL_effect-true_effect))/(np.abs(true_effect))\n",
    "\n",
    "        #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "        ape_2[index_1, 0, p_[:,index]>=0.05] = 0\n",
    "        ape_2[index_1, 1, p_[:,index]>=0.05] = 0\n",
    "        ape_2[index_1, 2, p_[:,index]>=0.05] = 0\n",
    "\n",
    "        se[index_1, 0, :] = ((add_effect - true_effect)**2)\n",
    "        se[index_1, 1, :] = ((LR_effect - true_effect)**2)\n",
    "        se[index_1, 2, :] = ((PDL_effect - true_effect)**2)\n",
    "\n",
    "        ae[index_1, 0, :] = np.abs((add_effect - true_effect))\n",
    "        ae[index_1, 1, :] = np.abs((LR_effect - true_effect))\n",
    "        ae[index_1, 2, :] = np.abs((PDL_effect - true_effect))\n",
    "    \n",
    "    \n",
    "        index_1 += 1\n",
    "\n",
    "    index += 1\n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of PDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of PDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE2 of LA         ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of LR         ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE2 of PDL        ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4247e",
   "metadata": {},
   "source": [
    "# Robustness to DNN Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f624fe40",
   "metadata": {},
   "source": [
    "## Data generating process definition\n",
    "define delta = 0.1, 0.2, 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea9c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:31:13.658089Z",
     "start_time": "2023-02-11T04:31:13.651547Z"
    }
   },
   "outputs": [],
   "source": [
    "err = 0.1#delta coefficient in the paper\n",
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "\n",
    "#No need to change\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "n_est = int(m*500)\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "estimator_0 = []#ATE by ML in base\n",
    "estimator_debias_0 = []#ATE by DML in base\n",
    "true_1 = []#real ATE in treatment\n",
    "estimator_1 = []#ATE by ML in treatment\n",
    "estimator_debias_1 = []#ATE by DML in treatment\n",
    "estimator_additive_1 = []#ATE by additive in treatment\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_DL = []\n",
    "p_DDL = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727964b0",
   "metadata": {},
   "source": [
    "## Comparison of LA, LR, SDL, DeDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ad252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:31:50.794763Z",
     "start_time": "2023-02-11T04:31:19.425827Z"
    }
   },
   "outputs": [],
   "source": [
    "for rep_index in range(n_rep):\n",
    "    print('# of replication:', rep_index)\n",
    "    #generate coef and biased coef\n",
    "    \n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "    \n",
    "    coef_ = coef.copy()\n",
    "    c_est = c_true*(1+np.random.uniform(-err, err))\n",
    "    d_est = 0\n",
    "    \n",
    "\n",
    "    def generate_y_true(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            y[i] = (c/(1+np.exp(-((x[i].dot(coef))).dot(t[i]))) + d + y_error[i])\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef, x, t):\n",
    "        return c_true/(1+np.exp(-((x.dot(coef))).dot(t)))\n",
    "    \n",
    "    \n",
    "    #generate samples for training LR\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y(coef, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est_xt = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est_xt, samples_y_est.reshape(n_est,1), axis=1)\n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_est,1), full_data_est_xt)\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y(coef, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est,1), axis=1)\n",
    "    \n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        \n",
    "\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "    \n",
    "        pred_y_loss = []\n",
    "        beta_ = []\n",
    "        for i in range(n_est):\n",
    "            beta_biased = samples_x_est[i].dot(coef_)*(1+np.random.uniform(-err, err, m+1))#add bias\n",
    "            beta_.append(beta_biased)  \n",
    "            pred_y_loss.append(c_est/(1+np.exp(-(beta_biased).dot(samples_t_est[i]))))\n",
    "        beta_ = np.array(beta_)\n",
    "        pred_y_loss = np.array(pred_y_loss)\n",
    "        real_y = samples_y_est.copy()\n",
    "        pred_y = []\n",
    "        for i in range(n_est): \n",
    "            pred_y.append(c_est/(1+np.exp(-(beta_[i]).dot(t_star_base))))\n",
    "        pred_y = np.array(pred_y)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "            \n",
    "            \n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            #G_theta.append(c_true*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            #G_theta_loss.append(c_true*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_combo_obs.shape[0]):\n",
    "                t = t_combo_obs[i]\n",
    "                y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                u = beta_temp.dot(t)\n",
    "                #G_prime = c_true*np.exp(-u)/(np.exp(-u)+1)**2*t\n",
    "                G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "                #lambda_inv.append(inv(lambda_))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_0.append(np.mean(real_y_star))\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))\n",
    "        estimator_0.append(np.mean(pred_y))\n",
    "        estimator_debias_0.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        pred_y_debiased_0 = pred_y_debiased.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        pred_y = []\n",
    "        for i in range(n_est): \n",
    "            pred_y.append(c_est/(1+np.exp(-(beta_[i]).dot(t_star_base))))\n",
    "        pred_y = np.array(pred_y)\n",
    "        \n",
    "\n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "\n",
    "        #lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            #for i in range(t_combo_obs.shape[0]):\n",
    "            #    t = t_combo_obs[i]\n",
    "            #    y = avg_y(beta_temp, c_true, d_true, t)\n",
    "            #    u = beta_temp.dot(t)\n",
    "            #    G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "            #    lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            #try:\n",
    "            #    lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "            #except:\n",
    "            #    print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_1.append(np.mean(real_y_star))        \n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        estimator_1.append(np.mean(pred_y))\n",
    "        estimator_debias_1.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        p_.append(stats.ttest_rel(real_y_star, real_y_star_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1])\n",
    "        p_DL.append(stats.ttest_rel(pred_y, pred_y_0)[1])\n",
    "        p_DDL.append(stats.ttest_rel(pred_y_debiased, pred_y_debiased_0)[1])\n",
    "        \n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e04614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:31:56.674454Z",
     "start_time": "2023-02-11T04:31:56.630226Z"
    }
   },
   "outputs": [],
   "source": [
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_DL = np.array(p_DL).reshape((n_rep, 2**m))\n",
    "p_DDL = np.array(p_DDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_0 = np.array(estimator_0).reshape((n_rep, 2**m))\n",
    "estimator_debias_0 = np.array(estimator_debias_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_1 = np.array(estimator_1).reshape((n_rep, 2**m))\n",
    "estimator_debias_1 = np.array(estimator_debias_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR SDL DeDL\n",
    "ape_2 = np.zeros([2**m, 4, n_rep])\n",
    "se = np.zeros([2**m, 4, n_rep])\n",
    "ae = np.zeros([2**m, 4, n_rep])\n",
    "CDR = np.zeros([2**m, 4, n_rep])\n",
    "for i in range(n_rep):\n",
    "    CDR[0,:,i] = 1\n",
    "    for j in range(1,2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        dl = estimator_1[i,j]-estimator_0[i,j]\n",
    "        dedl = estimator_debias_1[i,j]-estimator_debias_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DL[i,j]<=0.05 and dl*true_eff>0) or (p_[i,j]>0.05 and p_DL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DDL[i,j]<=0.05 and dedl*true_eff>0) or (p_[i,j]>0.05 and p_DDL[i,j]>0.05):\n",
    "            CDR[j,3,i] = 1\n",
    "            \n",
    "\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]    \n",
    "    no_bias_effect = estimator_1[:,index]-estimator_0[:,index]\n",
    "    debias_effect = estimator_debias_1[:,index]-estimator_debias_0[:,index]\n",
    "    \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((no_bias_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 3, :] = np.abs((debias_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 3, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)    \n",
    "    se[index, 2, :] = ((no_bias_effect - true_effect)**2)\n",
    "    se[index, 3, :] = ((debias_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((no_bias_effect - true_effect))\n",
    "    ae[index, 3, :] = np.abs((debias_effect - true_effect))\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of SDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('MSE of DeDL         ','|', np.mean(se[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,3,:], axis=0)), scale=st.sem(np.mean(se[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of SDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('MAE of DeDL         ','|', np.mean(ae[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,3,:], axis=0)), scale=st.sem(np.mean(ae[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE of LA          ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of LR          ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of SDL         ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of DeDL        ','|', np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of SDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('CDR of DeDL         ','|', np.mean(CDR[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,3,:], axis=0)), scale=st.sem(np.mean(CDR[:,3,:], axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 4])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(5)\n",
    "    max_effect_index = np.zeros(5)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        no_bias_effect = estimator_1[i,index]-estimator_0[i,index]\n",
    "        debias_effect = estimator_debias_1[i,index]-estimator_debias_0[i,index]\n",
    "        \n",
    "        if true_effect > max_effect[4]:\n",
    "            max_effect[4] = true_effect\n",
    "            max_effect_index[4] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if no_bias_effect > max_effect[2]:\n",
    "            max_effect[2] = no_bias_effect\n",
    "            max_effect_index[2] = index\n",
    "        if debias_effect > max_effect[3]:\n",
    "            max_effect[3] = debias_effect\n",
    "            max_effect_index[3] = index\n",
    "        \n",
    "    if max_effect_index[0] == max_effect_index[4]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[4]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[4]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "    if max_effect_index[3] == max_effect_index[4]:\n",
    "        cnt_bai[i, 3] = 1\n",
    "    \n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of SDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('BAI of DeDL         ','|', np.mean(cnt_bai[:, 3]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 3]), scale=st.sem(cnt_bai[:, 3])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e853a",
   "metadata": {},
   "source": [
    "# Link Function Misspecification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f807dab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T17:53:40.076064Z",
     "start_time": "2023-02-10T17:53:26.359568Z"
    }
   },
   "outputs": [],
   "source": [
    "y_linear = []\n",
    "y_sigmoid = []\n",
    "n_cnver_test = 10000\n",
    "gamma_ = 1\n",
    "\n",
    "rep_index = 0\n",
    "while rep_index < 100:\n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    coef_linear = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "    \n",
    "    def generate_y_true_linear(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            y[i] = gamma_*(x[i].dot(coef_linear)).dot(t[i])  \n",
    "        return y\n",
    "    \n",
    "    def generate_y_true_sigmoid(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            y[i] =  c/(1+np.exp(-((x[i].dot(coef))).dot(t[i])))\n",
    "        return y\n",
    "                     \n",
    "    samples_x_cnver = generate_x(d_c, n_cnver_test)\n",
    "    samples_t_cnver = generate_t(t_combo, t_dist, n_cnver_test)\n",
    "    y_linear.append(generate_y_true_linear(coef, c_true, d_true, samples_x_cnver, samples_t_cnver, n_cnver_test))\n",
    "    y_sigmoid.append(generate_y_true_sigmoid(coef, c_true, d_true, samples_x_cnver, samples_t_cnver, n_cnver_test))\n",
    "    rep_index+=1\n",
    "    #print(rep_index)\n",
    "y_linear = np.array(y_linear).reshape(-1)\n",
    "y_sigmoid = np.array(y_sigmoid).reshape(-1)    \n",
    "#plt.hist(y_linear, label='linear', density=True, alpha=0.6,bins=20)\n",
    "#plt.hist(y_sigmoid, label='sigmoid', density=True, alpha=0.6,bins=20)\n",
    "#plt.xlabel('HTE')\n",
    "#plt.xlim(-20, 20)\n",
    "#plt.ylabel('density')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "#print('HTE Linear     ','|', np.mean(y_linear),'|',st.t.interval(0.95, len(y_linear)-1, loc=np.mean(y_linear), scale=st.sem(y_linear)))\n",
    "#print('HTE Sigmoid    ','|', np.mean(y_sigmoid),'|',st.t.interval(0.95, len(y_sigmoid)-1, loc=np.mean(y_sigmoid), scale=st.sem(y_sigmoid)))\n",
    "from IPython.core.pylabtools import figsize\n",
    "from matplotlib.ticker import FixedFormatter\n",
    "figsize(6, 4)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.rcParams['savefig.dpi'] = 1200\n",
    "plt.rcParams['figure.dpi'] = 1200\n",
    "ax.yaxis.set_ticks_position('both')\n",
    "ax.xaxis.set_ticks_position('both')\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rc('font',family='Times New Roman')\n",
    "y_linear_1 = y_linear + y_sigmoid\n",
    "y_linear_2 = 3*y_linear+y_sigmoid\n",
    "y_linear_5 = 5*y_linear+y_sigmoid\n",
    "#y_linear_1 = y_linear \n",
    "#y_linear_2 = 2*y_linear\n",
    "#y_linear_5 = 5*y_linear\n",
    "sns.set_theme(style=\"white\",font=\"Times New Roman\")\n",
    "\n",
    "sns.kdeplot(y_sigmoid, label='\\u03B3 = 0',fill=True,bw_adjust=5)\n",
    "sns.kdeplot(y_linear_1[np.where((y_linear_1 > -10) & (y_linear_1 < 30))],color=\"r\", label='\\u03B3 = 1',fill=True,bw_adjust=5)\n",
    "sns.kdeplot(y_linear_2[np.where((y_linear_2 > -10) & (y_linear_2 < 30))],color=\"g\", label='\\u03B3 = 3',fill=True,bw_adjust=5)\n",
    "sns.kdeplot(y_linear_5[np.where((y_linear_5 > -10) & (y_linear_5 < 30))],color=\"orange\", label='\\u03B3 = 5',fill=True,bw_adjust=5)\n",
    "plt.xlabel('Outcome Y')\n",
    "plt.xlim(-10, 30)\n",
    "plt.ylabel('density')\n",
    "plt.legend(loc=\"upper left\")\n",
    "#plt.savefig('misspecific_HTE.png', dpi=600, bbox_inches='tight',pad_inches=0.0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9d28a",
   "metadata": {},
   "source": [
    "## Data generating process definition\n",
    "define the level of gamma = 0, 1, 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364669a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:32:02.637548Z",
     "start_time": "2023-02-11T04:32:02.630371Z"
    }
   },
   "outputs": [],
   "source": [
    "gamma_ = 1\n",
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "\n",
    "n_est = int(m*500)\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "#reg_loss = 0.001\n",
    "reg_loss = 0\n",
    "train_epochs = 2000\n",
    "test_thres = 0.2*gamma_+0.1\n",
    "if gamma_ == 0:\n",
    "    test_thres = 0.03\n",
    "    \n",
    "n_cnver_test = 10000\n",
    "lr = 0.05\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "estimator_0 = []#ATE by SDL in base\n",
    "estimator_debias_0 = []#ATE by DEDL in base\n",
    "\n",
    "true_1 = []#real ATE in treatment\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "estimator_1 = []#ATE by SDL in treatment\n",
    "estimator_debias_1 = []#ATE by DEDL in treatment\n",
    "\n",
    "estimator_additive_1 = []#ATE by additive in treatment\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_DL = []\n",
    "p_DDL = []\n",
    "test_err = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec4937",
   "metadata": {},
   "source": [
    "## Comparison of LA, LR, SDL, DeDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da448c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:34:08.984786Z",
     "start_time": "2023-02-11T04:32:08.583560Z"
    }
   },
   "outputs": [],
   "source": [
    "rep_index = 0\n",
    "\n",
    "while rep_index < n_rep:\n",
    "    print('# of replication:', rep_index)\n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    coef_linear = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "    \n",
    "\n",
    "    def generate_y_true(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            y[i] = gamma_*(x[i].dot(coef_linear)).dot(t[i]) + (c/(1+np.exp(-((x[i].dot(coef))).dot(t[i]))) + d + y_error[i])\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef, x, t):\n",
    "        return gamma_*(x.dot(coef_linear)).dot(t) + c_true/(1+np.exp(-((x.dot(coef))).dot(t)))\n",
    "\n",
    "    \n",
    "    #generate samples for estimation\n",
    "    samples_x_est = generate_x(d_c, n_train)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_train)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_train)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_train,1), axis=1)\n",
    "    data_obs = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "\n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_train,1), np.append(samples_x_est, samples_t_est, axis=1))\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    train_samples = int(0.7*data_obs.shape[0])\n",
    "    x_train_set = np.float32(data_obs)[0:train_samples, 0:-1]\n",
    "    y_train_set = np.float32(data_obs)[0:train_samples, -1]\n",
    "    x_test_set = np.float32(data_obs)[train_samples:, 0:-1]\n",
    "    y_test_set = np.float32(data_obs)[train_samples:, -1]\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=opt.batch_size_test, shuffle=True)\n",
    "    net = FNN_asig()\n",
    "    test_accuracy = 100\n",
    "    print('---------warm-up train---------')\n",
    "    wp_cnt = 0\n",
    "    while(test_accuracy >= 1):\n",
    "        wp_cnt += 1\n",
    "        net = FNN_asig()\n",
    "        with torch.no_grad():\n",
    "            net.layer3.weight[0, 0] = float(np.max(y_train_set))\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.L1Loss()\n",
    "        #warm-up train\n",
    "        test_accuracy = 100\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.1, weight_decay = opt.wd)\n",
    "        for epoch in range(200):\n",
    "            for i, data_i in enumerate(trainloader, 0):\n",
    "                inputs, labels = data_i\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "                loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "            test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "            #if epoch%100 == 0:\n",
    "            #    print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "            #        epoch, train_accuracy, test_accuracy))\n",
    "            if test_accuracy < 1:\n",
    "                break\n",
    "        if wp_cnt >= 5:\n",
    "            break\n",
    "    #training \n",
    "    print('---------train---------')\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = opt.wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(train_epochs):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "        test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "                epoch, train_accuracy, test_accuracy))\n",
    "        if test_accuracy < test_thres and epoch > 500:\n",
    "            break  \n",
    "    if test_accuracy > test_thres:\n",
    "        continue\n",
    "    else:\n",
    "        rep_index += 1\n",
    "        test_err.append(test_accuracy)\n",
    "    #rep_index += 1\n",
    "    \n",
    "    activation = {}\n",
    "    net.layer1.register_forward_hook(get_activation('layer1'))\n",
    "\n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est,1), axis=1)\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        beta_ = []\n",
    "        pred_y_loss = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            beta_.append(activation['layer1'].tolist())  \n",
    "            pred_y_loss.append(outputs.tolist())\n",
    "        beta_ = np.array(beta_).reshape(n_est, m+1)\n",
    "        pred_y_loss = np.array(pred_y_loss).reshape(n_est)\n",
    "        \n",
    "        real_y = samples_y_est.copy()\n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_combo_obs.shape[0]):\n",
    "                t = t_combo_obs[i]\n",
    "                y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                u = beta_temp.dot(t)\n",
    "                G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_0.append(np.mean(real_y_star))\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))\n",
    "        estimator_0.append(np.mean(pred_y))\n",
    "        estimator_debias_0.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        pred_y_debiased_0 = pred_y_debiased.copy()\n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        for i in range(n_est):\n",
    "            for j in range(m):\n",
    "                x_all_set[i][-m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "\n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_1.append(np.mean(real_y_star))\n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        estimator_1.append(np.mean(pred_y))\n",
    "        estimator_debias_1.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        p_.append(stats.ttest_ind(real_y_star, real_y_star_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1]) \n",
    "        p_DL.append(stats.ttest_ind(pred_y, pred_y_0)[1])\n",
    "        p_DDL.append(stats.ttest_ind(pred_y_debiased, pred_y_debiased_0)[1])\n",
    "        \n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e34ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:34:14.870076Z",
     "start_time": "2023-02-11T04:34:14.824707Z"
    }
   },
   "outputs": [],
   "source": [
    "test_err_np = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    test_err_np[i] = test_err[i].tolist()\n",
    "test_err =  test_err_np.copy()\n",
    "del test_err_np\n",
    "\n",
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_DL = np.array(p_DL).reshape((n_rep, 2**m))\n",
    "p_DDL = np.array(p_DDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_0 = np.array(estimator_0).reshape((n_rep, 2**m))\n",
    "estimator_debias_0 = np.array(estimator_debias_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_1 = np.array(estimator_1).reshape((n_rep, 2**m))\n",
    "estimator_debias_1 = np.array(estimator_debias_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR SDL DeDL\n",
    "ape_2 = np.zeros([2**m, 4, n_rep])\n",
    "se = np.zeros([2**m, 4, n_rep])\n",
    "ae = np.zeros([2**m, 4, n_rep])\n",
    "CDR = np.zeros([2**m, 4, n_rep])\n",
    "for i in range(n_rep):\n",
    "    CDR[0,:,i] = 1\n",
    "    for j in range(1,2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        dl = estimator_1[i,j]-estimator_0[i,j]\n",
    "        dedl = estimator_debias_1[i,j]-estimator_debias_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DL[i,j]<=0.05 and dl*true_eff>0) or (p_[i,j]>0.05 and p_DL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DDL[i,j]<=0.05 and dedl*true_eff>0) or (p_[i,j]>0.05 and p_DDL[i,j]>0.05):\n",
    "            CDR[j,3,i] = 1\n",
    "            \n",
    "\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]    \n",
    "    no_bias_effect = estimator_1[:,index]-estimator_0[:,index]\n",
    "    debias_effect = estimator_debias_1[:,index]-estimator_debias_0[:,index]\n",
    "    \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((no_bias_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 3, :] = np.abs((debias_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 3, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)    \n",
    "    se[index, 2, :] = ((no_bias_effect - true_effect)**2)\n",
    "    se[index, 3, :] = ((debias_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((no_bias_effect - true_effect))\n",
    "    ae[index, 3, :] = np.abs((debias_effect - true_effect))\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of SDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('MSE of DeDL         ','|', np.mean(se[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,3,:], axis=0)), scale=st.sem(np.mean(se[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of SDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('MAE of DeDL         ','|', np.mean(ae[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,3,:], axis=0)), scale=st.sem(np.mean(ae[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE of LA          ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of LR          ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of SDL         ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of DeDL        ','|', np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of SDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('CDR of DeDL         ','|', np.mean(CDR[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,3,:], axis=0)), scale=st.sem(np.mean(CDR[:,3,:], axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 4])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(5)\n",
    "    max_effect_index = np.zeros(5)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        no_bias_effect = estimator_1[i,index]-estimator_0[i,index]\n",
    "        debias_effect = estimator_debias_1[i,index]-estimator_debias_0[i,index]\n",
    "        \n",
    "        if true_effect > max_effect[4]:\n",
    "            max_effect[4] = true_effect\n",
    "            max_effect_index[4] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if no_bias_effect > max_effect[2]:\n",
    "            max_effect[2] = no_bias_effect\n",
    "            max_effect_index[2] = index\n",
    "        if debias_effect > max_effect[3]:\n",
    "            max_effect[3] = debias_effect\n",
    "            max_effect_index[3] = index\n",
    "        \n",
    "    if max_effect_index[0] == max_effect_index[4]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[4]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[4]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "    if max_effect_index[3] == max_effect_index[4]:\n",
    "        cnt_bai[i, 3] = 1\n",
    "    \n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of SDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('BAI of DeDL         ','|', np.mean(cnt_bai[:, 3]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 3]), scale=st.sem(cnt_bai[:, 3])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b78b53",
   "metadata": {},
   "source": [
    "## Model Misspecified - High Order Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_ = 1\n",
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "\n",
    "n_est = int(m*500)\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "#reg_loss = 0.001\n",
    "reg_loss = 0\n",
    "train_epochs = 2000\n",
    "test_thres = 0.2*gamma_+0.1\n",
    "if gamma_ == 0:\n",
    "    test_thres = 0.03\n",
    "    \n",
    "n_cnver_test = 10000\n",
    "lr = 0.05\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "estimator_0 = []#ATE by SDL in base\n",
    "estimator_debias_0 = []#ATE by DEDL in base\n",
    "\n",
    "true_1 = []#real ATE in treatment\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "estimator_1 = []#ATE by SDL in treatment\n",
    "estimator_debias_1 = []#ATE by DEDL in treatment\n",
    "\n",
    "estimator_additive_1 = []#ATE by additive in treatment\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_DL = []\n",
    "p_DDL = []\n",
    "test_err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e57c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_index = 0\n",
    "\n",
    "while rep_index < n_rep:\n",
    "    print('# of replication:', rep_index)\n",
    "    coef_linear = np.random.uniform(0, 0.5, [d_c, 2**m-11])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "    \n",
    "\n",
    "    def generate_y_true(coef_linear, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            t_ = t[i]\n",
    "            t_ext = np.append(t_, np.array([t_[1]*t_[2], t_[1]*t_[3], t_[1]*t_[4], t_[2]*t_[3], t_[2]*t_[4], t_[3]*t_[4],\n",
    "                       t_[1]*t_[2]*t_[3], t_[2]*t_[3]*t_[4], t_[3]*t_[4]*t_[1], t_[4]*t_[1]*t_[2], \n",
    "                       t_[1]*t_[2]*t_[3]*t_[4]]))\n",
    "            y[i] = gamma_*(x[i].dot(coef_linear)).dot(t_ext[0:-11]) + y_error[i]\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef_linear, x, t_):\n",
    "        t_ext = np.append(t_, np.array([t_[1]*t_[2], t_[1]*t_[3], t_[1]*t_[4], t_[2]*t_[3], t_[2]*t_[4], t_[3]*t_[4],\n",
    "                       t_[1]*t_[2]*t_[3], t_[2]*t_[3]*t_[4], t_[3]*t_[4]*t_[1], t_[4]*t_[1]*t_[2], \n",
    "                       t_[1]*t_[2]*t_[3]*t_[4]]))\n",
    "        return gamma_*(x.dot(coef_linear)).dot(t_ext[0:-11])\n",
    "\n",
    "    \n",
    "    #generate samples for estimation\n",
    "    samples_x_est = generate_x(d_c, n_train)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_train)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef_linear, c_true, d_true, samples_x_est, samples_t_est, n_train)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_train,1), axis=1)\n",
    "    data_obs = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "\n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_train,1), np.append(samples_x_est, samples_t_est, axis=1))\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "    \n",
    "    opt = parser.parse_args()\n",
    "    train_samples = int(0.7*data_obs.shape[0])\n",
    "    x_train_set = np.float32(data_obs)[0:train_samples, 0:-1]\n",
    "    y_train_set = np.float32(data_obs)[0:train_samples, -1]\n",
    "    x_test_set = np.float32(data_obs)[train_samples:, 0:-1]\n",
    "    y_test_set = np.float32(data_obs)[train_samples:, -1]\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=opt.batch_size_test, shuffle=True)\n",
    "    net = FNN_asig()\n",
    "    test_accuracy = 100\n",
    "    print('---------warm-up train---------')\n",
    "    wp_cnt = 0\n",
    "    while(test_accuracy >= 1):\n",
    "        wp_cnt += 1\n",
    "        net = FNN_asig()\n",
    "        with torch.no_grad():\n",
    "            net.layer3.weight[0, 0] = float(np.max(y_train_set))\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.L1Loss()\n",
    "        #warm-up train\n",
    "        test_accuracy = 100\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.1, weight_decay = opt.wd)\n",
    "        for epoch in range(200):\n",
    "            for i, data_i in enumerate(trainloader, 0):\n",
    "                inputs, labels = data_i\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "                loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "            test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "            #if epoch%100 == 0:\n",
    "            #    print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "            #        epoch, train_accuracy, test_accuracy))\n",
    "            if test_accuracy < 1:\n",
    "                break\n",
    "        if wp_cnt >= 5:\n",
    "            break\n",
    "    #training \n",
    "    print('---------train---------')\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = opt.wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(train_epochs):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "        test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "                epoch, train_accuracy, test_accuracy))\n",
    "        if test_accuracy < test_thres and epoch > 500:\n",
    "            break  \n",
    "    if test_accuracy > test_thres:\n",
    "        continue\n",
    "    else:\n",
    "        rep_index += 1\n",
    "        test_err.append(test_accuracy)\n",
    "    #rep_index += 1\n",
    "    \n",
    "    activation = {}\n",
    "    net.layer1.register_forward_hook(get_activation('layer1'))\n",
    "    true_value=[]\n",
    "    for name, param in net.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            true_value.append(param.data)\n",
    "    c_est = np.array(true_value[-1].tolist())[0][0]\n",
    "    d_est = 0\n",
    "\n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x(d_c, n_est)\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef_linear, c_true, d_true, samples_x_est, samples_t_est, n_est)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est,1), axis=1)\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        \n",
    "        beta_ = []\n",
    "        pred_y_loss = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            beta_.append(activation['layer1'].tolist())  \n",
    "            pred_y_loss.append(outputs.tolist())\n",
    "        beta_ = np.array(beta_).reshape(n_est, m+1)\n",
    "        pred_y_loss = np.array(pred_y_loss).reshape(n_est)\n",
    "        \n",
    "        real_y = samples_y_est.copy()\n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef_linear,samples_x_est[i],t_star_base))\n",
    "\n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_combo_obs.shape[0]):\n",
    "                t = t_combo_obs[i]\n",
    "                y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                u = beta_temp.dot(t)\n",
    "                G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_0.append(np.mean(real_y_star))\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))\n",
    "        estimator_0.append(np.mean(pred_y))\n",
    "        estimator_debias_0.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        pred_y_debiased_0 = pred_y_debiased.copy()\n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        for i in range(n_est):\n",
    "            for j in range(m):\n",
    "                x_all_set[i][-m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=1000, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est)\n",
    "        \n",
    "\n",
    "        real_y_star = []\n",
    "        for i in range(n_est):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef_linear,samples_x_est[i],t_star_base))\n",
    "\n",
    "\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "\n",
    "        true_1.append(np.mean(real_y_star))\n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        estimator_1.append(np.mean(pred_y))\n",
    "        estimator_debias_1.append(np.mean(pred_y_debiased))\n",
    "        \n",
    "        p_.append(stats.ttest_ind(real_y_star, real_y_star_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1]) \n",
    "        p_DL.append(stats.ttest_ind(pred_y, pred_y_0)[1])\n",
    "        p_DDL.append(stats.ttest_ind(pred_y_debiased, pred_y_debiased_0)[1])\n",
    "        \n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ae200",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_err_np = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    test_err_np[i] = test_err[i].tolist()\n",
    "test_err =  test_err_np.copy()\n",
    "del test_err_np\n",
    "\n",
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_DL = np.array(p_DL).reshape((n_rep, 2**m))\n",
    "p_DDL = np.array(p_DDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_0 = np.array(estimator_0).reshape((n_rep, 2**m))\n",
    "estimator_debias_0 = np.array(estimator_debias_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_1 = np.array(estimator_1).reshape((n_rep, 2**m))\n",
    "estimator_debias_1 = np.array(estimator_debias_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR SDL DeDL\n",
    "ape_2 = np.zeros([2**m, 4, n_rep])\n",
    "se = np.zeros([2**m, 4, n_rep])\n",
    "ae = np.zeros([2**m, 4, n_rep])\n",
    "CDR = np.zeros([2**m, 4, n_rep])\n",
    "for i in range(n_rep):\n",
    "    CDR[0,:,i] = 1\n",
    "    for j in range(1,2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        dl = estimator_1[i,j]-estimator_0[i,j]\n",
    "        dedl = estimator_debias_1[i,j]-estimator_debias_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DL[i,j]<=0.05 and dl*true_eff>0) or (p_[i,j]>0.05 and p_DL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DDL[i,j]<=0.05 and dedl*true_eff>0) or (p_[i,j]>0.05 and p_DDL[i,j]>0.05):\n",
    "            CDR[j,3,i] = 1\n",
    "            \n",
    "\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]    \n",
    "    no_bias_effect = estimator_1[:,index]-estimator_0[:,index]\n",
    "    debias_effect = estimator_debias_1[:,index]-estimator_debias_0[:,index]\n",
    "    \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((no_bias_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 3, :] = np.abs((debias_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 3, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)    \n",
    "    se[index, 2, :] = ((no_bias_effect - true_effect)**2)\n",
    "    se[index, 3, :] = ((debias_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((no_bias_effect - true_effect))\n",
    "    ae[index, 3, :] = np.abs((debias_effect - true_effect))\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of SDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('MSE of DeDL         ','|', np.mean(se[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,3,:], axis=0)), scale=st.sem(np.mean(se[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of SDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('MAE of DeDL         ','|', np.mean(ae[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,3,:], axis=0)), scale=st.sem(np.mean(ae[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE of LA          ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of LR          ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of SDL         ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of DeDL        ','|', np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of SDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('CDR of DeDL         ','|', np.mean(CDR[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,3,:], axis=0)), scale=st.sem(np.mean(CDR[:,3,:], axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 4])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(5)\n",
    "    max_effect_index = np.zeros(5)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        no_bias_effect = estimator_1[i,index]-estimator_0[i,index]\n",
    "        debias_effect = estimator_debias_1[i,index]-estimator_debias_0[i,index]\n",
    "        \n",
    "        if true_effect > max_effect[4]:\n",
    "            max_effect[4] = true_effect\n",
    "            max_effect_index[4] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if no_bias_effect > max_effect[2]:\n",
    "            max_effect[2] = no_bias_effect\n",
    "            max_effect_index[2] = index\n",
    "        if debias_effect > max_effect[3]:\n",
    "            max_effect[3] = debias_effect\n",
    "            max_effect_index[3] = index\n",
    "        \n",
    "    if max_effect_index[0] == max_effect_index[4]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[4]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[4]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "    if max_effect_index[3] == max_effect_index[4]:\n",
    "        cnt_bai[i, 3] = 1\n",
    "    \n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of SDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('BAI of DeDL         ','|', np.mean(cnt_bai[:, 3]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 3]), scale=st.sem(cnt_bai[:, 3])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7520b",
   "metadata": {},
   "source": [
    "# Imbalanced Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600ee51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-10T18:15:46.310749Z",
     "start_time": "2023-02-10T18:15:46.293567Z"
    }
   },
   "source": [
    "## Data generating process definition\n",
    "define the level of lambda = 2, 1, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641582fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:34:20.840449Z",
     "start_time": "2023-02-11T04:34:20.830487Z"
    }
   },
   "outputs": [],
   "source": [
    "lambda_level = 2\n",
    "precision_reb = 1/2 #precision of rebalance, precision_reb=1 means no rebalancing\n",
    "\n",
    "\n",
    "m = 4  #number of experiments\n",
    "d_c = 10 #number of features\n",
    "\n",
    "def generate_x_imb(d_c, n):\n",
    "    x = np.random.uniform(0, 1, [n, d_c])\n",
    "    x[:,-1] = np.random.exponential(1.0/lambda_level, n)\n",
    "    return x\n",
    "\n",
    "def re_x(x_1, prec=1):\n",
    "    x_1_b = x_1.copy()\n",
    "    if prec == 1:\n",
    "        return x_1_b\n",
    "    else:\n",
    "        n_bucket = int(1/prec)\n",
    "        n_samples = x_1_b.shape[0]\n",
    "        min_n = 10000\n",
    "        for lb in np.arange(0,1,prec):\n",
    "            rb = lb + prec\n",
    "            min_n = min(min_n, (np.greater(x_1_b[:,-1],lb)*np.less(x_1_b[:,-1],rb)).sum())\n",
    "        x_emp = []\n",
    "        for lb in np.arange(0,1,prec):\n",
    "            rb = lb + prec\n",
    "            x_temp = x_1_b[np.greater(x_1_b[:,-1],lb)*np.less(x_1_b[:,-1],rb),:]\n",
    "            x_index = np.random.choice(x_temp.shape[0], min_n, replace=False)\n",
    "            x_emp.append(x_temp[x_index])\n",
    "        return np.array(x_emp).reshape(min_n*n_bucket, x_1_b.shape[1])\n",
    "\n",
    "\n",
    "all_combo = list(powerset(list(np.arange(1, m+1))))\n",
    "t_combo = []\n",
    "for i in all_combo:\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo.append(t)\n",
    "t_combo = np.int16(t_combo)\n",
    "t_dist = (1/2**m)*np.ones(2**m)\n",
    "t_combo_obs = []\n",
    "for i in range(m+1):\n",
    "    t = np.zeros(m+1)\n",
    "    t[0] = 1\n",
    "    t[i] = 1\n",
    "    t_combo_obs.append(t)\n",
    "t = np.zeros(m+1)\n",
    "t[[0,1,2]] = 1\n",
    "t_combo_obs.append(t)\n",
    "t_combo_obs = np.int16(t_combo_obs)\n",
    "t_dist_obs = (1/(m+2))*np.ones(m+2)\n",
    "\n",
    "t_star_all = t_combo.copy()\n",
    "idx = (t_combo[:,None]!=t_combo_obs).any(-1).all(1)\n",
    "t_star_unobs = t_combo[idx]\n",
    "\n",
    "\n",
    "n_est = int(m*500)\n",
    "n_train = int(m*500)\n",
    "reg_term = 0.0005\n",
    "reg_loss = 0\n",
    "train_epochs = 2000\n",
    "test_thres = 0.5\n",
    "lr = 0.05\n",
    "\n",
    "feature_list = []\n",
    "for i in range(d_c):\n",
    "    feature_list.append(str('x')+str(i+1))\n",
    "t_list = []\n",
    "for i in range(m+1):\n",
    "    t_list.append(str('t')+str(i))\n",
    "\n",
    "true_0 = []#real ATE in base\n",
    "estimator_0 = []#ATE by ML in base\n",
    "estimator_debias_0 = []#ATE by DML in base\n",
    "estimator_LR_0 = []#ATE by LR in base\n",
    "\n",
    "true_1 = []#real ATE in treatment\n",
    "estimator_1 = []#ATE by ML in treatment\n",
    "estimator_debias_1 = []#ATE by DML in treatment\n",
    "estimator_LR_1 = []#ATE by LR in treatment\n",
    "\n",
    "estimator_additive_1 = []#ATE by additive in treatment\n",
    "\n",
    "p_ = []\n",
    "p_LA = []\n",
    "p_LR = []\n",
    "p_DL = []\n",
    "p_DDL = []\n",
    "test_err = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25e7e5",
   "metadata": {},
   "source": [
    "## Comparison of LA, LR, SDL, DeDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55516121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:35:29.323324Z",
     "start_time": "2023-02-11T04:34:26.773694Z"
    }
   },
   "outputs": [],
   "source": [
    "rep_index = 0\n",
    "\n",
    "while rep_index < n_rep:\n",
    "    \n",
    "    print('# of replication:', rep_index)\n",
    "    coef = np.random.uniform(-0.5, 0.5, [d_c, m+1])\n",
    "    c_true = np.random.uniform(10, 20)\n",
    "    d_true = 0\n",
    "    \n",
    "    def generate_y_true(coef, c, d, x, t, n):\n",
    "        y = np.zeros(n)\n",
    "        y_error = 0.05*np.random.uniform(-1, 1, n)\n",
    "        for i in range(n):\n",
    "            y[i] = c/(1+np.exp(-((x[i].dot(coef))).dot(t[i]))) + d + y_error[i]\n",
    "        return y, y_error\n",
    "    def generate_y_true_1(coef, x, t):\n",
    "        return c_true/(1+np.exp(-((x.dot(coef))).dot(t)))\n",
    "    def generate_beta_true_1(coef, x, t):\n",
    "        return (x.dot(coef))\n",
    "    \n",
    "    #generate samples for estimation\n",
    "    samples_x_est = generate_x_imb(d_c, n_train)\n",
    "    samples_x_est = re_x(samples_x_est, precision_reb)\n",
    "    n_train_ = samples_x_est.shape[0]\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_train_)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_train_)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_train_,1), axis=1)\n",
    "    data_obs = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "    \n",
    "    model_LR = sm.OLS(samples_y_est.reshape(n_train_,1), np.append(samples_x_est, samples_t_est, axis=1))\n",
    "    results_LR = model_LR.fit()\n",
    "    coef_LR = results_LR.params\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    train_samples = int(0.7*data_obs.shape[0])\n",
    "    x_train_set = np.float32(data_obs)[0:train_samples, 0:-1]\n",
    "    y_train_set = np.float32(data_obs)[0:train_samples, -1]\n",
    "    x_test_set = np.float32(data_obs)[train_samples:, 0:-1]\n",
    "    y_test_set = np.float32(data_obs)[train_samples:, -1]\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(torch.Tensor(x_train_set), torch.Tensor(y_train_set)) # create your datset\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=opt.batch_size_train, shuffle=True)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(torch.Tensor(x_test_set), torch.Tensor(y_test_set)) # create your datset\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=opt.batch_size_test, shuffle=True)\n",
    "    net = FNN_asig()\n",
    "    test_accuracy = 100\n",
    "    print('---------warm-up train---------')\n",
    "    wp_cnt = 0\n",
    "    while(test_accuracy >= 1):\n",
    "        wp_cnt += 1\n",
    "        net = FNN_asig()\n",
    "        with torch.no_grad():\n",
    "            net.layer3.weight[0, 0] = float(np.max(y_train_set))\n",
    "        criterion = nn.MSELoss()\n",
    "        #criterion = nn.L1Loss()\n",
    "        #warm-up train\n",
    "        test_accuracy = 100\n",
    "        optimizer = optim.Adam(net.parameters(), lr = 0.1, weight_decay = opt.wd)\n",
    "        for epoch in range(200):\n",
    "            for i, data_i in enumerate(trainloader, 0):\n",
    "                inputs, labels = data_i\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "                loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            #train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "            test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "            #if epoch%100 == 0:\n",
    "            #    print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "            #        epoch, train_accuracy, test_accuracy))\n",
    "            if test_accuracy < 1:\n",
    "                break\n",
    "        if wp_cnt >= 5:\n",
    "            break\n",
    "    #training \n",
    "    print('---------train---------')\n",
    "    optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = opt.wd)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(train_epochs):\n",
    "        for i, data_i in enumerate(trainloader, 0):\n",
    "            inputs, labels = data_i\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "            loss = criterion(outputs, labels) + reg_loss*l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = calculate_mse(trainloader, opt.is_gpu, net)\n",
    "        test_accuracy = calculate_mse(testloader, opt.is_gpu, net)\n",
    "        if epoch%100 == 0:\n",
    "            print(\"Iteration: {0} | Training MSE: {1} | Test MSE: {2}\".format(\n",
    "                epoch, train_accuracy, test_accuracy))\n",
    "        if test_accuracy < test_thres and epoch > 500:\n",
    "            break  \n",
    "    if test_accuracy > test_thres:\n",
    "        continue\n",
    "    else:\n",
    "        rep_index += 1\n",
    "        test_err.append(test_accuracy)\n",
    "    #rep_index += 1\n",
    "    \n",
    "    activation = {}\n",
    "    net.layer1.register_forward_hook(get_activation('layer1'))\n",
    "    \n",
    "    #generate samples for inference\n",
    "    samples_x_est = generate_x_imb(d_c, n_est)\n",
    "    samples_x_est = re_x(samples_x_est, precision_reb)\n",
    "    n_est_ = samples_x_est.shape[0]\n",
    "    samples_t_est = generate_t(t_combo_obs, t_dist_obs, n_est_)\n",
    "    samples_y_est, samples_y_error_est = generate_y_true(coef, c_true, d_true, samples_x_est, samples_t_est, n_est_)\n",
    "    full_data_est = np.append(samples_x_est, samples_t_est, axis=1)\n",
    "    full_data_est = np.append(full_data_est, samples_y_est.reshape(n_est_,1), axis=1)\n",
    "\n",
    "    for t_star in t_star_all:\n",
    "        data_est = pd.DataFrame(full_data_est, columns = feature_list + t_list + ['y'])\n",
    "        #at t=0\n",
    "        t_star_base = np.zeros(m+1)\n",
    "        t_star_base[0] = 1\n",
    "        x_all_set=np.float32(data_est)[:, 0:-1]\n",
    "        y_all_set=np.float32(data_est)[:, -1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=n_est_, shuffle=False)\n",
    "        \n",
    "        beta_ = []\n",
    "        pred_y_loss = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            beta_.append(activation['layer1'].tolist())  \n",
    "            pred_y_loss.append(outputs.tolist())\n",
    "        beta_ = np.array(beta_).reshape(n_est_, m+1)\n",
    "        pred_y_loss = np.array(pred_y_loss).reshape(n_est_)\n",
    "        \n",
    "        real_y = samples_y_est.copy()\n",
    "        \n",
    "        for j in range(m):\n",
    "            x_all_set[:, -m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=n_est_, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est_)\n",
    "        \n",
    "        #real_y_star = []\n",
    "        #for i in range(n_est):\n",
    "        #    real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est[i],t_star_base))\n",
    "\n",
    "        lambda_inv = []\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "            for i in range(t_combo_obs.shape[0]):\n",
    "                t = t_combo_obs[i]\n",
    "                y = avg_y(beta_temp, c_true, d_true, t)\n",
    "                u = beta_temp.dot(t)\n",
    "                G_prime = np.append(c_est*np.exp(-u)/(np.exp(-u)+1)**2*t, [1/(np.exp(-u)+1)])\n",
    "                lambda_ += t_dist_obs[i]*2*np.outer(G_prime, G_prime)\n",
    "\n",
    "            try:\n",
    "                lambda_inv.append(inv(lambda_ + reg_term*np.eye(m+2)))\n",
    "            except:\n",
    "                print('Singular matrix')\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est_):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est_):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "        \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est_): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "        \n",
    "        \n",
    "        samples_x_est_true = generate_x(d_c, 10000)\n",
    "        real_y_star = []\n",
    "        for i in range(10000):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est_true[i],t_star_base))\n",
    "        \n",
    "        \n",
    "\n",
    "        estimator_0.append(np.mean(pred_y))\n",
    "        estimator_debias_0.append(np.mean(pred_y_debiased))\n",
    "        true_0.append(np.mean(real_y_star))\n",
    "        estimator_LR_0.append(np.mean(pred_y_LR))\n",
    "\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_LR_0 = pred_y_LR.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        \n",
    "        #record\n",
    "        real_y_star_0 = real_y_star.copy()\n",
    "        pred_y_0 = pred_y.copy()\n",
    "        pred_y_debiased_0 = pred_y_debiased.copy()\n",
    "        \n",
    "        #at t=t_star\n",
    "        t_star_base = t_star.copy()\n",
    "        \n",
    "        for i in range(n_est_):\n",
    "            for j in range(m):\n",
    "                x_all_set[i][-m+j] = t_star_base[j+1]\n",
    "        allset = torch.utils.data.TensorDataset(torch.Tensor(x_all_set), torch.Tensor(y_all_set)) # create your datset\n",
    "        allloader = torch.utils.data.DataLoader(\n",
    "            allset, batch_size=n_est_, shuffle=False)\n",
    "        pred_y = []\n",
    "        for i, data_ in enumerate(allloader, 0):\n",
    "            inputs, labels = data_\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = net(inputs)\n",
    "            pred_y.append(outputs.tolist())\n",
    "        pred_y = np.array(pred_y).reshape(n_est_)\n",
    "        \n",
    "\n",
    "        G_theta = []\n",
    "        G_theta_loss = []\n",
    "        t_obs_ = samples_t_est.copy()\n",
    "        cnt = 0\n",
    "\n",
    "        for beta_temp in beta_:\n",
    "\n",
    "            lambda_ = np.zeros([m+2, m+2])#asig use m+2\n",
    "            u_ = beta_temp.dot(t_star_base)\n",
    "            G_theta.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*t_star_base, [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            u_ = beta_temp.dot(samples_t_est[cnt])\n",
    "            G_theta_loss.append(np.append(c_est*np.exp(-u_)/(np.exp(-u_)+1)**2*samples_t_est[cnt], [1/(np.exp(-u_)+1)]))\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "        lambda_inv_loss_prime = []\n",
    "        for i in range(n_est_):\n",
    "            lambda_inv_loss_prime.append(2*(pred_y_loss[i]-real_y[i])*lambda_inv[i].dot(G_theta_loss[i]))\n",
    "        lambda_inv_loss_prime = np.array(lambda_inv_loss_prime)\n",
    "\n",
    "        pred_y_debiased = []\n",
    "        for i in range(n_est_):\n",
    "            pred_y_debiased.append(pred_y[i]-G_theta[i].dot(lambda_inv_loss_prime[i]))\n",
    "            \n",
    "        pred_y_LR = []\n",
    "        for i in range(n_est_): \n",
    "            pred_y_LR.append(np.append(samples_x_est[i], t_star_base).dot(coef_LR))\n",
    "        pred_y_LR = np.array(pred_y_LR)\n",
    "        \n",
    "       \n",
    "        samples_x_est_true = generate_x(d_c, 10000)\n",
    "        real_y_star = []\n",
    "        for i in range(10000):\n",
    "            real_y_star.append(0.05*np.random.uniform(-1, 1)+generate_y_true_1(coef,samples_x_est_true[i],t_star_base))\n",
    "        \n",
    "        \n",
    "        estimator_1.append(np.mean(pred_y))\n",
    "        estimator_debias_1.append(np.mean(pred_y_debiased))\n",
    "        estimator_LR_1.append(np.mean(pred_y_LR))\n",
    "        \n",
    "        true_1.append(np.mean(real_y_star))\n",
    "        p_.append(stats.ttest_ind(real_y_star, real_y_star_0)[1])\n",
    "        p_DL.append(stats.ttest_ind(pred_y, pred_y_0)[1])\n",
    "        p_DDL.append(stats.ttest_ind(pred_y_debiased, pred_y_debiased_0)[1])\n",
    "        p_LR.append(stats.ttest_ind(pred_y_LR, pred_y_LR_0)[1]) \n",
    "        \n",
    "        #calculate additive\n",
    "        pred_y_additive = 0\n",
    "        pred_y_additive_var = 0\n",
    "        n_LA_samples = 0\n",
    "        for single_exp_index in np.where(t_star == 1)[0]:\n",
    "            ttt = np.zeros(m+1)\n",
    "            ttt[0] = 1\n",
    "            ttt[single_exp_index] = 1\n",
    "            pred_y_additive += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "            pred_y_additive_var += samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].var()\n",
    "            n_LA_samples += (samples_t_est == ttt).sum()\n",
    "        ttt = np.zeros(m+1)\n",
    "        ttt[0] = 1\n",
    "        pred_y_additive -= (np.sum(t_star))*samples_y_est[np.where((samples_t_est == ttt).all(axis=1))].mean()\n",
    "        estimator_additive_1.append(pred_y_additive)\n",
    "        t_value = pred_y_additive/np.sqrt(pred_y_additive_var*(np.sum(t_star))/n_LA_samples)\n",
    "        p_LA.append(stats.t.sf(abs(t_value), df=1))\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b1417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-11T04:35:32.919749Z",
     "start_time": "2023-02-11T04:35:32.868686Z"
    }
   },
   "outputs": [],
   "source": [
    "test_err_np = np.zeros(n_rep)\n",
    "for i in range(n_rep):\n",
    "    test_err_np[i] = test_err[i].tolist()\n",
    "test_err =  test_err_np.copy()\n",
    "del test_err_np\n",
    "\n",
    "p_ = np.array(p_).reshape((n_rep, 2**m))\n",
    "p_LA = np.array(p_LA).reshape((n_rep, 2**m))\n",
    "p_LR = np.array(p_LR).reshape((n_rep, 2**m))\n",
    "p_DL = np.array(p_DL).reshape((n_rep, 2**m))\n",
    "p_DDL = np.array(p_DDL).reshape((n_rep, 2**m))\n",
    "true_0 = np.array(true_0).reshape((n_rep, 2**m))\n",
    "estimator_LR_0 = np.array(estimator_LR_0).reshape((n_rep, 2**m))\n",
    "estimator_0 = np.array(estimator_0).reshape((n_rep, 2**m))\n",
    "estimator_debias_0 = np.array(estimator_debias_0).reshape((n_rep, 2**m))\n",
    "true_1 = np.array(true_1).reshape((n_rep, 2**m))\n",
    "estimator_LR_1 = np.array(estimator_LR_1).reshape((n_rep, 2**m))\n",
    "estimator_1 = np.array(estimator_1).reshape((n_rep, 2**m))\n",
    "estimator_debias_1 = np.array(estimator_debias_1).reshape((n_rep, 2**m))\n",
    "estimator_additive_1 = np.array(estimator_additive_1).reshape((n_rep, 2**m))\n",
    "index = 0\n",
    "\n",
    "#LA LR SDL DeDL\n",
    "ape_2 = np.zeros([2**m, 4, n_rep])\n",
    "se = np.zeros([2**m, 4, n_rep])\n",
    "ae = np.zeros([2**m, 4, n_rep])\n",
    "CDR = np.zeros([2**m, 4, n_rep])\n",
    "for i in range(n_rep):\n",
    "    CDR[0,:,i] = 1\n",
    "    for j in range(1,2**m):\n",
    "        true_eff = true_1[i,j]-true_0[i,j]\n",
    "        la = estimator_additive_1[i,j]\n",
    "        lr = estimator_LR_1[i,j]-estimator_LR_0[i,j]\n",
    "        dl = estimator_1[i,j]-estimator_0[i,j]\n",
    "        dedl = estimator_debias_1[i,j]-estimator_debias_0[i,j]\n",
    "        if (p_[i,j]<=0.05 and p_LA[i,j]<=0.05 and la*true_eff>0) or (p_[i,j]>0.05 and p_LA[i,j]>0.05):\n",
    "            CDR[j,0,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_LR[i,j]<=0.05 and lr*true_eff>0) or (p_[i,j]>0.05 and p_LR[i,j]>0.05):\n",
    "            CDR[j,1,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DL[i,j]<=0.05 and dl*true_eff>0) or (p_[i,j]>0.05 and p_DL[i,j]>0.05):\n",
    "            CDR[j,2,i] = 1\n",
    "        if (p_[i,j]<=0.05 and p_DDL[i,j]<=0.05 and dedl*true_eff>0) or (p_[i,j]>0.05 and p_DDL[i,j]>0.05):\n",
    "            CDR[j,3,i] = 1\n",
    "            \n",
    "\n",
    "print('----------------All combos--------------------')\n",
    "for t_star in t_star_all:\n",
    "    #print('Treatment effect when t = ', t_star[1:])\n",
    "    true_effect = true_1[:,index]-true_0[:,index]\n",
    "    add_effect = estimator_additive_1[:,index]\n",
    "    LR_effect = estimator_LR_1[:,index]-estimator_LR_0[:,index]    \n",
    "    no_bias_effect = estimator_1[:,index]-estimator_0[:,index]\n",
    "    debias_effect = estimator_debias_1[:,index]-estimator_debias_0[:,index]\n",
    "    \n",
    "    ape_2[index, 0, :] = np.abs((add_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 1, :] = np.abs((LR_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 2, :] = np.abs((no_bias_effect-true_effect))/(np.abs(true_effect))\n",
    "    ape_2[index, 3, :] = np.abs((debias_effect-true_effect))/(np.abs(true_effect))\n",
    "    \n",
    "    #if p_[:,index]>=0.05:#not significant treatment effect\n",
    "    ape_2[index, 0, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 1, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 2, p_[:,index]>=0.05] = 0\n",
    "    ape_2[index, 3, p_[:,index]>=0.05] = 0\n",
    "    \n",
    "    se[index, 0, :] = ((add_effect - true_effect)**2)\n",
    "    se[index, 1, :] = ((LR_effect - true_effect)**2)    \n",
    "    se[index, 2, :] = ((no_bias_effect - true_effect)**2)\n",
    "    se[index, 3, :] = ((debias_effect - true_effect)**2)\n",
    "    \n",
    "    ae[index, 0, :] = np.abs((add_effect - true_effect))\n",
    "    ae[index, 1, :] = np.abs((LR_effect - true_effect))\n",
    "    ae[index, 2, :] = np.abs((no_bias_effect - true_effect))\n",
    "    ae[index, 3, :] = np.abs((debias_effect - true_effect))\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "print('------------------------------------')\n",
    "print('MSE of LA           ','|', np.mean(se[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,0,:], axis=0)), scale=st.sem(np.mean(se[:,0,:], axis=0))))\n",
    "print('MSE of LR           ','|', np.mean(se[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,1,:], axis=0)), scale=st.sem(np.mean(se[:,1,:], axis=0))))\n",
    "print('MSE of SDL          ','|', np.mean(se[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,2,:], axis=0)), scale=st.sem(np.mean(se[:,2,:], axis=0))))\n",
    "print('MSE of DeDL         ','|', np.mean(se[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(se[:,3,:], axis=0)), scale=st.sem(np.mean(se[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAE of LA           ','|', np.mean(ae[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,0,:], axis=0)), scale=st.sem(np.mean(ae[:,0,:], axis=0))))\n",
    "print('MAE of LR           ','|', np.mean(ae[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,1,:], axis=0)), scale=st.sem(np.mean(ae[:,1,:], axis=0))))\n",
    "print('MAE of SDL          ','|', np.mean(ae[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,2,:], axis=0)), scale=st.sem(np.mean(ae[:,2,:], axis=0))))\n",
    "print('MAE of DeDL         ','|', np.mean(ae[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ae[:,3,:], axis=0)), scale=st.sem(np.mean(ae[:,3,:], axis=0))))\n",
    "print('------------------------------------')\n",
    "print('MAPE of LA          ','|', np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,0,:]*2**m/np.sum(ape_2[:,0,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of LR          ','|', np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,1,:]*2**m/np.sum(ape_2[:,1,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of SDL         ','|', np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,2,:]*2**m/np.sum(ape_2[:,2,:]!=0, axis=0), axis=0))))\n",
    "print('MAPE of DeDL        ','|', np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0)), \n",
    "                                                                          scale=st.sem(np.mean(ape_2[:,3,:]*2**m/np.sum(ape_2[:,3,:]!=0, axis=0), axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "print('CDR of LA           ','|', np.mean(CDR[:,0,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,0,:], axis=0)), scale=st.sem(np.mean(CDR[:,0,:], axis=0))))\n",
    "print('CDR of LR           ','|', np.mean(CDR[:,1,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,1,:], axis=0)), scale=st.sem(np.mean(CDR[:,1,:], axis=0))))\n",
    "print('CDR of SDL          ','|', np.mean(CDR[:,2,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,2,:], axis=0)), scale=st.sem(np.mean(CDR[:,2,:], axis=0))))\n",
    "print('CDR of DeDL         ','|', np.mean(CDR[:,3,:]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(np.mean(CDR[:,3,:], axis=0)), scale=st.sem(np.mean(CDR[:,3,:], axis=0))))\n",
    "\n",
    "print('------------------------------------')\n",
    "\n",
    "cnt_bai = np.zeros([n_rep, 4])\n",
    "for i in range(n_rep):\n",
    "    max_effect = -10000*np.ones(5)\n",
    "    max_effect_index = np.zeros(5)\n",
    "    for index in range(2**m):\n",
    "        true_effect = true_1[i,index]-true_0[i,index]\n",
    "        add_effect = estimator_additive_1[i,index]\n",
    "        LR_effect = estimator_LR_1[i,index]-estimator_LR_0[i,index]\n",
    "        no_bias_effect = estimator_1[i,index]-estimator_0[i,index]\n",
    "        debias_effect = estimator_debias_1[i,index]-estimator_debias_0[i,index]\n",
    "        \n",
    "        if true_effect > max_effect[4]:\n",
    "            max_effect[4] = true_effect\n",
    "            max_effect_index[4] = index\n",
    "        if add_effect > max_effect[0]:\n",
    "            max_effect[0] = add_effect\n",
    "            max_effect_index[0] = index\n",
    "        if LR_effect > max_effect[1]:\n",
    "            max_effect[1] = LR_effect\n",
    "            max_effect_index[1] = index\n",
    "        if no_bias_effect > max_effect[2]:\n",
    "            max_effect[2] = no_bias_effect\n",
    "            max_effect_index[2] = index\n",
    "        if debias_effect > max_effect[3]:\n",
    "            max_effect[3] = debias_effect\n",
    "            max_effect_index[3] = index\n",
    "        \n",
    "    if max_effect_index[0] == max_effect_index[4]:\n",
    "        cnt_bai[i, 0] = 1\n",
    "    if max_effect_index[1] == max_effect_index[4]:\n",
    "        cnt_bai[i, 1] = 1\n",
    "    if max_effect_index[2] == max_effect_index[4]:\n",
    "        cnt_bai[i, 2] = 1\n",
    "    if max_effect_index[3] == max_effect_index[4]:\n",
    "        cnt_bai[i, 3] = 1\n",
    "    \n",
    "print('BAI of LA           ','|', np.mean(cnt_bai[:, 0]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 0]), scale=st.sem(cnt_bai[:, 0])))\n",
    "print('BAI of LR           ','|', np.mean(cnt_bai[:, 1]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 1]), scale=st.sem(cnt_bai[:, 1])))\n",
    "print('BAI of SDL          ','|', np.mean(cnt_bai[:, 2]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 2]), scale=st.sem(cnt_bai[:, 2])))\n",
    "print('BAI of DeDL         ','|', np.mean(cnt_bai[:, 3]),'|',st.t.interval(0.95, n_rep-1, loc=np.mean(cnt_bai[:, 3]), scale=st.sem(cnt_bai[:, 3])))\n",
    "print('------------------------------------')\n",
    "print('ABS of ATE          ','|', np.mean(np.mean(np.abs(true_1-true_0), axis=1)),'|',st.t.interval(0.95, n_rep-1, \n",
    "                                                                                            loc=np.mean(np.mean(np.abs(true_1-true_0), axis=1)), \n",
    "                                                                                            scale=st.sem(np.mean(np.abs(true_1-true_0), axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e8428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
